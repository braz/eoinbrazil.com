<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Insights - Blog/Python</title><link href="http://eoinbrazil.com/" rel="alternate"></link><link href="http://eoinbrazil.com/feeds/blogpython.atom.xml" rel="self"></link><id>http://eoinbrazil.com/</id><updated>2018-03-31T00:00:00+01:00</updated><entry><title>Celery Database Bottlenecks</title><link href="http://eoinbrazil.com/celerydatabasebottlenecks.html" rel="alternate"></link><published>2018-03-31T00:00:00+01:00</published><updated>2018-03-31T00:00:00+01:00</updated><author><name>Eoin Brazil</name></author><id>tag:eoinbrazil.com,2018-03-31:/celerydatabasebottlenecks.html</id><summary type="html">&lt;p&gt;I recently had to refactor some code which uses MongoDB and Celery to store results from a scraping process to a MongoDB collection. It involved a number of whack a mole type performance problems due to the distributed nature of the system, and indeed was leading to the &lt;a href="https://linux-mm.org/OOM_Killer"&gt;Linux out …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently had to refactor some code which uses MongoDB and Celery to store results from a scraping process to a MongoDB collection. It involved a number of whack a mole type performance problems due to the distributed nature of the system, and indeed was leading to the &lt;a href="https://linux-mm.org/OOM_Killer"&gt;Linux out of memory (&lt;span class="caps"&gt;OOM&lt;/span&gt;) killer&lt;/a&gt; being triggered against some of those workers. I wanted to write up some of the approaches I took as they may be helpful to others and indeed maybe there are better ways out there to handle the same situation (so any feedback would be much&amp;nbsp;appreciated!).&lt;/p&gt;
&lt;p&gt;The inspiration for writing this approach in a blog was another &lt;a href="https://www.vinta.com.br/blog/2018/dealing-resource-consuming-tasks-celery/"&gt;blog post on handling memory intensive Celery workers&lt;/a&gt;. This highlighted yet another Celery setting I wasn&amp;#8217;t using on &lt;a href="http://docs.celeryproject.org/en/latest/userguide/optimizing.html#prefetch-limits"&gt;prefetch limits&lt;/a&gt; or indeed aware of. The approach from that blog used Celery&amp;#8217;s worker prefetch limiting to avoid running out of memory on the workers. The approach below was what I used to avoid running out of memory. It occurs at one step earlier by avoiding sending tasks to the Celery worker from the Python programme distributing the work than throttling the tasks once on the workers. I think either approach may work and I&amp;#8217;m glad to have found the blog to both prompt future thinking and my writing of this&amp;nbsp;post.&lt;/p&gt;
&lt;h2&gt;1. Limit task submission if Database queue is at or above the maximum&amp;nbsp;threshold&lt;/h2&gt;
&lt;p&gt;This function implemented a simple limit and wait logic for the application calling the specific Celery Workers, the example below uses a trivialised example with a single Worker named &amp;#8216;celery1&amp;#8217;. The Celery task inspection function, celery.task.control.inspect(), is queried in the calling Python programme and the running-threads value for the specific worker (which is only used for this one queue) is what determines whether this function will return. If there are too many existing tasks on the worker, the function will sleep until the number of tasks are less than the value set for the variable,&amp;nbsp;message_queue_max.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;check_and_backoff_if_db_queue_at_max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;message_queue_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;
&lt;span class="n"&gt;ins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inspect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;queue_over_max_length_for_worker_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="n"&gt;all_stats&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ins&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stats&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;all_stats&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;No Celery Workers have been detected.&amp;quot;&lt;/span&gt;
            &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;RuntimeError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;celery1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;all_stats&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pool&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;running-threads&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;message_queue_max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;queue_over_max_length_for_worker_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;queue_over_max_length_for_worker_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;The DB queue is above &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; pausing for 15 seconds.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message_queue_max&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;kombu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exceptions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OperationalError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Failed to connect to Celery Workers or to RabbitMQ at {}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;celery_config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;broker_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;RuntimeError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h2&gt;2. Setting appropriate task settings for this type of &lt;span class="caps"&gt;DB&lt;/span&gt;&amp;nbsp;task&lt;/h2&gt;
&lt;p&gt;In order to better manage this type of task, I set three of Celery&amp;#8217;s Task setting. The results are ignored from the task, the task was configured for only acknowledged late rather than using the default of acknowledge on receipt rather than completion of the task (acks_late), and finally the time limits for this task was removed. The task is also configured to make two retry attempts if it&amp;nbsp;fails.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;max_retries&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ignore_result&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;acks_late&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;soft_time_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;3. Changing the database usage from a upsert to a insert and ignore&amp;nbsp;approach&lt;/h2&gt;
&lt;p&gt;This step actually removed the need for the backoff for my particular usage where a large number of upserts operations (updates for existing documents or if not present then the document(s) are inserted) were the root cause of the Celery Worker having such a long queue. A change to a insert only approach, which was feasible for my application as it only required the document to be present once, provided close on two orders of magnitute improvement to the database operations and significantly reduced the Celery Worker queue as tasks were no longer backing&amp;nbsp;up.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bulk_result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MongoClient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mongo_uri&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DATABASE&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;COLLECTION&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bulk_write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;array_of_insert_one_operations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ordered&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;4. Minor discoveries in a useful practical sense but not core to solving my&amp;nbsp;problem&lt;/h2&gt;
&lt;p&gt;In the reading and research to address my queue growing too large and triggering the &lt;a href="https://linux-mm.org/OOM_Killer"&gt;Linux out of memory (&lt;span class="caps"&gt;OOM&lt;/span&gt;) killer&lt;/a&gt;, I found two useful Celery setting which I had not been aware of. Firstly, you can setup Celery Workers to receive events using the (&amp;#8220;-E&amp;#8221;) option which allows for restarting worker pools directly through Celery Flower (the Management &lt;span class="caps"&gt;UI&lt;/span&gt; I use for Celery) when combined with the &amp;#8220;worker_pool_restarts&amp;#8221;&amp;nbsp;setting.&lt;/p&gt;
&lt;p&gt;A second aspect of our internal tooling using a scraper approach, specifically a number of the workers use tasks that are primarily focused on asychronous &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests so these use the &lt;a href="http://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html"&gt;Eventlet execution pool&lt;/a&gt;. The deployment uses a standard Celery/RabbitMQ configuration. RabbitMQ uses a default setting where 10 concurrent connections are kept open for the broker pool when used with Celery. The setting &amp;#8220;broker_pool_limit&amp;#8221; allows for this to be raised, in this example below it is set to 100 which is more suitable for this type of Celery Worker/Eventlet execution pool&amp;nbsp;combination.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;worker_pool_restarts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;span class="n"&gt;broker_pool_limit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Future&amp;nbsp;directions&lt;/h2&gt;
&lt;p&gt;The tooling currently uses &lt;a href="http://docs.celeryproject.org/en/latest/reference/celery.bin.multi.html"&gt;celery.bin.multi&lt;/a&gt; with bash scripting as each node hosts multiple workers, however it is being containerized so I think my next blog will cover moving to &lt;a href="http://supervisord.org/"&gt;supervisord&lt;/a&gt; with a Celery setup of multiple workers per node providing multiple&amp;nbsp;queues.&lt;/p&gt;</content><category term="python"></category><category term="queues"></category><category term="celery"></category><category term="bottlenecks"></category></entry><entry><title>Two approaches to scale your processing: Task Queues and Workflows</title><link href="http://eoinbrazil.com/queuesandworkflows.html" rel="alternate"></link><published>2017-09-14T00:00:00+01:00</published><updated>2017-09-14T00:00:00+01:00</updated><author><name>Eoin Brazil</name></author><id>tag:eoinbrazil.com,2017-09-14:/queuesandworkflows.html</id><summary type="html">&lt;p&gt;I was very fortunate to present to the main track at PyCon Ireland 2017 on the topic of scaling your&amp;nbsp;processing.&lt;/p&gt;
&lt;iframe width="640" height="360" src="https://www.youtube.com/embed/OY_hGTwDaSE?list=PLNeBS51Q0m98FAWRHwQLjV1vja2ZdWpCb" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The talk was entitled &lt;a href="https://docs.google.com/presentation/d/1W7WvodRej6A3XAPYWOCS6dZam_z5DV2gXhLimKRrGbM/edit?usp=sharing"&gt;&amp;#8220;Two approaches to scale your processing: Task Queues and Workflows&amp;#8221;&lt;/a&gt; and was aimed at an intermediate Python audience to introduce them to the Celery and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was very fortunate to present to the main track at PyCon Ireland 2017 on the topic of scaling your&amp;nbsp;processing.&lt;/p&gt;
&lt;iframe width="640" height="360" src="https://www.youtube.com/embed/OY_hGTwDaSE?list=PLNeBS51Q0m98FAWRHwQLjV1vja2ZdWpCb" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The talk was entitled &lt;a href="https://docs.google.com/presentation/d/1W7WvodRej6A3XAPYWOCS6dZam_z5DV2gXhLimKRrGbM/edit?usp=sharing"&gt;&amp;#8220;Two approaches to scale your processing: Task Queues and Workflows&amp;#8221;&lt;/a&gt; and was aimed at an intermediate Python audience to introduce them to the Celery and Airflow tools for queues and for workflows respectively. It was more mainstream talk than the data science track I&amp;#8217;d prevoiusly presented at and I was able to talk about many of my own learnings building tooling with&amp;nbsp;MongoDB.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://eoinbrazil.com/pdfs/PyCon2017-TwoApproachesToScaleYourProcessing.pdf"&gt;Pycon 2017 Talk &lt;span class="caps"&gt;PDF&lt;/span&gt;&amp;nbsp;version&lt;/a&gt;&lt;/p&gt;
&lt;script async class="speakerdeck-embed" data-id="3fa87c9829184c4189d612283a5c9040" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"&gt;&lt;/script&gt;</content><category term="python"></category><category term="presentations"></category><category term="data science"></category><category term="queues"></category><category term="celery"></category><category term="airflow"></category><category term="rabbitmq"></category></entry><entry><title>An introduction to Gradient Boosting</title><link href="http://eoinbrazil.com/gradientboostingwithpython.html" rel="alternate"></link><published>2016-10-07T00:00:00+01:00</published><updated>2016-10-07T00:00:00+01:00</updated><author><name>Eoin Brazil</name></author><id>tag:eoinbrazil.com,2016-10-07:/gradientboostingwithpython.html</id><summary type="html">&lt;p&gt;In 2016 I again had the pleasure of presenting to the annual Python Ireland Conference or PyCon. I gave a talk on &amp;#8220;Gradient Boosting&amp;#8221; which was aimed at introducing this technique in terms of general data science aspects but also the Python libraries that one can ues for&amp;nbsp;it.&lt;/p&gt;
&lt;iframe width="600" height="380" src="https://www.youtube.com/embed/W7ZgagPAaxI?list=PLNeBS51Q0m98rI6kyTd5_-33ckRgjUEG3" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In 2016 I again had the pleasure of presenting to the annual Python Ireland Conference or PyCon. I gave a talk on &amp;#8220;Gradient Boosting&amp;#8221; which was aimed at introducing this technique in terms of general data science aspects but also the Python libraries that one can ues for&amp;nbsp;it.&lt;/p&gt;
&lt;iframe width="600" height="380" src="https://www.youtube.com/embed/W7ZgagPAaxI?list=PLNeBS51Q0m98rI6kyTd5_-33ckRgjUEG3" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;The source and examples for this talk are in a Github repo &lt;a href="https://github.com/braz/pycon2016_talk"&gt;PyCon 2016&lt;/a&gt; as well as some setup instructions to replicate my&amp;nbsp;examples.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://eoinbrazil.com/pdfs/PyCon2015-PythonMongoDBDataPipelines-Keynote.pdf"&gt;Pycon 2016 Talk &lt;span class="caps"&gt;PDF&lt;/span&gt;&amp;nbsp;version&lt;/a&gt;&lt;/p&gt;
&lt;script async class="speakerdeck-embed" data-id="5f40e5cb272d41b9b38de535d90fb6dc" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"&gt;&lt;/script&gt;</content><category term="python"></category><category term="presentations"></category><category term="data science"></category><category term="gradient boosting"></category><category term="scikit-learn"></category><category term="xgboost"></category></entry><entry><title>Data Pipelines with Python and MongoDB</title><link href="http://eoinbrazil.com/datapipelineswithpythonandmongodb.html" rel="alternate"></link><published>2015-10-08T00:00:00+01:00</published><updated>2015-10-08T00:00:00+01:00</updated><author><name>Eoin Brazil</name></author><id>tag:eoinbrazil.com,2015-10-08:/datapipelineswithpythonandmongodb.html</id><summary type="html">&lt;p&gt;I had the pleasure of presenting at PyCon 2015 and also at MongoDB Days &lt;span class="caps"&gt;UK&lt;/span&gt; 2015 on building data pipelines with Python and&amp;nbsp;MongoDB. &lt;/p&gt;
&lt;iframe width="600" height="380" src="https://www.youtube.com/embed/Mjbff6oPyhI?list=PLNeBS51Q0m99ud63wiwrPzbp6rwMqYprP" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Both of my talks source&amp;#8217;s and examples are in Github repos, &lt;a href="https://github.com/braz/pycon2015_talk"&gt;PyCon 2015&lt;/a&gt; and &lt;a href="https://github.com/braz/mongodbdays2015_talk"&gt;MongoDB Days &lt;span class="caps"&gt;UK&lt;/span&gt; 2015&lt;/a&gt;. The PyCon talk was aimed at introducing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I had the pleasure of presenting at PyCon 2015 and also at MongoDB Days &lt;span class="caps"&gt;UK&lt;/span&gt; 2015 on building data pipelines with Python and&amp;nbsp;MongoDB. &lt;/p&gt;
&lt;iframe width="600" height="380" src="https://www.youtube.com/embed/Mjbff6oPyhI?list=PLNeBS51Q0m99ud63wiwrPzbp6rwMqYprP" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Both of my talks source&amp;#8217;s and examples are in Github repos, &lt;a href="https://github.com/braz/pycon2015_talk"&gt;PyCon 2015&lt;/a&gt; and &lt;a href="https://github.com/braz/mongodbdays2015_talk"&gt;MongoDB Days &lt;span class="caps"&gt;UK&lt;/span&gt; 2015&lt;/a&gt;. The PyCon talk was aimed at introducing how you can use Python for building a data pipeline where MongoDB was the source for your&amp;nbsp;data. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://eoinbrazil.com/pdfs/PyCon2015-PythonMongoDBDataPipelines-Keynote.pdf"&gt;Pycon 2015 Talk &lt;span class="caps"&gt;PDF&lt;/span&gt;&amp;nbsp;version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://speakerdeck.com/braz/pycon-2015-using-mongodb-and-python-for-data-analysis-p"&gt;MongoDB Days &lt;span class="caps"&gt;UK&lt;/span&gt; talk&lt;/a&gt; but was aimed at more advanced MongoDB&amp;nbsp;audience.&lt;/p&gt;
&lt;script async class="speakerdeck-embed" data-id="048a946bfac14d409b9bf6ad1efa50e5" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"&gt;&lt;/script&gt;</content><category term="python"></category><category term="presentations"></category><category term="data pipeline"></category><category term="mongodb"></category><category term="data science"></category><category term="machine learning"></category><category term="scikit-learn"></category></entry></feed>